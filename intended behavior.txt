A description of my intentions with how this grab script should behave, to forestall a portion of the potential "is this a bug?" questions:

A "user directory" (more normally might be called a homepage) is in the form http://[server].upp.so-net.ne.jp/[user], where server is "www001", "www002", etc.; and user is the username. The server is unique to the user; a user on www016 does not exist on www001. Items are in the form userdir:[server]/[user], e.g. userdir:www016/onukiakira .

The script starts on the user's homepage as given above, with a few variations (with/without trailing slash, with/without index.html and index.htm). From here it recurses normally around the site. The only custom URL extraction, besides the normal cross-project extraction, is an aggressive URL extraction that extracts all strings consisting of a quote (not matched), followed by a series of URL-looking characters, followed by ".html", ".htm", or ".swf", followed by a quote (not matched); then queues the match if it looks like a valid URL in the current user's directory (including what look like a relative link). This has the potential to be dangerous on dynamic or large sites, but this is a static site, and in testing it got few false positives. (Also, the potential of this to spuriously extract many URLs from large binary files is in large part obviated by So-Net's storage limits.)

When Wget extracts links with its native link extraction (this does not apply to the aggressive custom URL extraction described above), they are queued even if they are external, as long as they are linked from, or are requisites of, a valid page in the item's user directory; when they are run, their requisites are in turn extracted and queued. This only fully recurses one level out; and then partially (only getting requisites) recurses two levels out.

Currently nothing is sent to backfeed, or sent back, except for warcs. It might be desirable to send discovered user directories elsewhere on So-Net U+ somewhere.
